import streamlit as st
import re
from collections import Counter

# --- –ò–ú–ü–û–†–¢ –ò –ù–ê–°–¢–†–û–ô–ö–ê STANA (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞) ---
try:
    import stanza
    # –ö—ç—à–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ –≥—Ä—É–∑–∏–ª–∞—Å—å –∫–∞–∂–¥—ã–π —Ä–∞–∑
    @st.cache_resource
    def load_stanza_pipeline():
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Stanza... (–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑)")
        stanza.download('kk')
        return stanza.Pipeline('kk', processors='tokenize,pos,lemma', verbose=False)

    nlp = load_stanza_pipeline()
    STANZA_AVAILABLE = True
except ImportError:
    st.error("–û—à–∏–±–∫–∞: 'stanza' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: pip install stanza")
    STANZA_AVAILABLE = False
except Exception as e:
    # –ï—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Stanza: {e}")
    STANZA_AVAILABLE = False

# ===============================================================
# –õ–û–ö–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –ê–ù–ê–õ–ò–ó–ê (–∏–∑ –≤–∞—à–µ–≥–æ .docx —Ñ–∞–π–ª–∞)
# ===============================================================

# –°–ª–æ–≤–∞—Ä—å –º–∞—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–∫–ª—é—á - –º–∞—Ä–∫–µ—Ä/—Å–æ—é–∑, –∑–Ω–∞—á–µ–Ω–∏–µ - —Ç–∏–ø)
RELATION_MARKERS = {
    # –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ (–ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–∞—è) [cite: 14]
    "”©–π—Ç–∫–µ–Ω—ñ": "1. –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "—Å–µ–±–µ–±—ñ": "1. –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "—Å–æ–Ω–¥—ã“õ—Ç–∞–Ω": "1. –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "—Å–æ–ª —Å–µ–±–µ–ø—Ç—ñ": "1. –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "–Ω–µ–≥–µ –¥–µ—Å–µ“£": "1. –°–µ–±–µ–ø-—Å–∞–ª–¥–∞—Ä–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    
    # “ö–∞—Ä–∞–º–∞-“õ–∞—Ä—Å—ã–ª—ã“õ / –£—Å—Ç—É–ø–∏—è–ª—ã“õ (–ü—Ä–æ—Ç–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è/–£—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–∞—è) [cite: 5]
    "–±—ñ—Ä–∞“õ": "5. “ö–∞—Ä–∞–º–∞-“õ–∞—Ä—Å—ã–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å—ã",
    "–∞–ª–∞–π–¥–∞": "5. “ö–∞—Ä–∞–º–∞-“õ–∞—Ä—Å—ã–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å—ã",
    "–¥–µ–≥–µ–Ω–º–µ–Ω": "4. –£—Å—Ç—É–ø–∏—è–ª—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "”ô–π—Ç–ø–µ—Å–µ": "2. –ü—Ä–æ—Ç–∏–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ—é–∑—ã", 
    
    # –®–∞—Ä—Ç—Ç—ã“õ (–£—Å–ª–æ–≤–Ω–∞—è) [cite: 17]
    "–µ–≥–µ—Ä": "3. –®–∞—Ä—Ç—Ç—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "–æ–Ω–¥–∞": "3. –®–∞—Ä—Ç—Ç—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    
    # “ö–æ—Ä—ã—Ç—ã–Ω–¥—ã (–í—ã–≤–æ–¥–Ω–∞—è) [cite: 15]
    "–¥–µ–º–µ–∫": "7. “ö–æ—Ä—ã—Ç—ã–Ω–¥—ã / —Ç“±–∂—ã—Ä—ã–º–¥—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    "–Ω”ô—Ç–∏–∂–µ—Å—ñ–Ω–¥–µ": "7. “ö–æ—Ä—ã—Ç—ã–Ω–¥—ã / —Ç“±–∂—ã—Ä—ã–º–¥—ã“õ –±–∞–π–ª–∞–Ω—ã—Å",
    
    # “ö–æ—Å—ã–º—à–∞ (–î–æ–±–∞–≤–æ—á–Ω–∞—è) [cite: 2]
    "–∂”ô–Ω–µ": "8. “ö–æ—Å—ã–º—à–∞ (—Ç–æ–ª—ã“õ—Ç—ã—Ä—É) –±–∞–π–ª–∞–Ω—ã—Å—ã",
    "”ô—Ä—ñ": "8. “ö–æ—Å—ã–º—à–∞ (—Ç–æ–ª—ã“õ—Ç—ã—Ä—É) –±–∞–π–ª–∞–Ω—ã—Å—ã",
    
    # –ú—ã—Å–∞–ª –∫–µ–ª—Ç—ñ—Ä—É (–ò–ª–ª—é—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è) [cite: 9]
    "–º—ã—Å–∞–ª—ã": "13. –ú—ã—Å–∞–ª –∫–µ–ª—Ç—ñ—Ä—É –±–∞–π–ª–∞–Ω—ã—Å—ã",
    
    # –¢“Ø—Å—ñ–Ω–¥—ñ—Ä–º–µ–ª—ñ (–ü–æ—è—Å–Ω–∏—Ç–µ–ª—å–Ω–∞—è) [cite: 9]
    "—è“ì–Ω–∏": "9. –¢“Ø—Å—ñ–Ω–¥—ñ—Ä–º–µ–ª—ñ –±–∞–π–ª–∞–Ω—ã—Å",
}

# –ü—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
STOP_WORDS = set([
    "–º–µ–Ω", "—Å–µ–Ω", "–æ–ª", "–±—ñ–∑", "—Å—ñ–∑", "–æ–ª–∞—Ä", "–∂”ô–Ω–µ", "–Ω–µ–º–µ—Å–µ", "–±—ñ—Ä–∞“õ", "–¥–µ–ø",
    "–¥–∞", "–¥–µ", "—Ç–∞", "—Ç–µ", "“Ø—à—ñ–Ω", "—Ç—É—Ä–∞–ª—ã", "–∞—Ä“õ—ã–ª—ã", "—Å–∞–π—ã–Ω", "–∫–µ–π—ñ–Ω", "–±–∞—Ä",
    "–∂–æ“õ", "–µ–º–µ—Å", "–¥–µ–¥—ñ", "–¥–µ–≥–µ–Ω", "–±—ñ—Ä", "–µ–∫—ñ", "“Ø—à", "–±“±–ª", "—Å–æ–ª", "–æ—Å—ã"
])

# ===============================================================
# –õ–û–ö–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –ê–ù–ê–õ–ò–ó–ê (–í–º–µ—Å—Ç–æ API)
# ===============================================================

def local_classify_relation(text):
    """
    –õ–æ–∫–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º.
    """
    text_lower = text.lower()
    found_relations = []
    
    for marker, relation_type in RELATION_MARKERS.items():
        if re.search(r'\b' + re.escape(marker) + r'\b', text_lower):
            if relation_type not in found_relations:
                found_relations.append(relation_type)
    
    if not found_relations:
        return "16. “ö“±—Ä—ã–ª—ã–º–¥—ã“õ / ”©—Ç–ø–µ–ª—ñ –±–∞–π–ª–∞–Ω—ã—Å (–ú–∞—Ä–∫–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)"
    
    return ", ".join(found_relations)

def local_extract_keywords(text):
    """
    –õ–æ–∫–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (Stanza + –ø–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã).
    """
    if not STANZA_AVAILABLE:
        return []
        
    doc = nlp(text)
    lemmas = []
    allowed_pos = {'NOUN', 'VERB', 'ADJ', 'PROPN'} # –°—É—â, –ì–ª–∞–≥–æ–ª, –ü—Ä–∏–ª, –ò–º—è
    
    for sent in doc.sentences:
        for word in sent.words:
            lemma = word.lemma.lower()
            if (word.pos in allowed_pos and 
                lemma not in STOP_WORDS and 
                len(lemma) > 2):
                lemmas.append(lemma)
                
    # –°—á–∏—Ç–∞–µ–º 5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ª–µ–º–º
    keyword_counts = Counter(lemmas)
    top_keywords = [lemma for lemma, count in keyword_counts.most_common(7)]
    return top_keywords

def generate_search_query(keywords):
    if not keywords:
        return ""
    return "Kazakh news search: " + " ".join(keywords)

def highlight_text(text, keywords):
    """
    –í—ã–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è HTML-–≤—ã–≤–æ–¥–∞ (markdown).
    """
    if not keywords:
        return text
    pattern = r'\b(' + '|'.join(re.escape(k) for k in keywords) + r')\b'
    highlighted = re.sub(pattern, r'<mark>\1</mark>', text, flags=re.IGNORECASE)
    return highlighted

# ===============================================================
# –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT
# ===============================================================

st.set_page_config(page_title="Kazakh NLP Analyzer (Local)", layout="wide")
st.title("üá∞üáø Kazakh NLP Analyzer (–õ–æ–∫–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)")

# 1. –ü–æ–ª–µ –≤–≤–æ–¥–∞
st.header("1. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ (–Ω–∞ –∫–∞–∑–∞—Ö—Å–∫–æ–º):")
example_news = "–ñ–æ–±–∞–¥–∞“ì—ã –º”ô–ª—ñ–º–µ—Ç—Ç–µ—Ä –¥”ô–ª–µ–ª–¥—ñ, —Å–æ–Ω–¥—ã“õ—Ç–∞–Ω “õ–æ—Ä—ã—Ç—ã–Ω–¥—ã –¥“±—Ä—ã—Å –¥–µ–ø –µ—Å–µ–ø—Ç–µ–ª–µ–¥—ñ. –î–µ–≥–µ–Ω–º–µ–Ω, –∫–µ–π–±—ñ—Ä —Å–∞—Ä–∞–ø—à—ã–ª–∞—Ä –æ–Ω—ã“£ —Ç–∏—ñ–º–¥—ñ–ª—ñ–≥—ñ–Ω–µ –±—ñ—Ä–∞“õ –∫“Ø–º”ô–Ω –∫–µ–ª—Ç—ñ—Ä—É–¥–µ."
news_text = st.text_area("–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:", value=example_news, height=150)

# 2. –ö–Ω–æ–ø–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
if st.button("‚ñ∂Ô∏è –õ–û–ö–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó"):
    if not STANZA_AVAILABLE:
        st.error("Stanza –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ê–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
    elif not news_text.strip():
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    else:
        # 3. –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        st.header("2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–õ–æ–∫–∞–ª—å–Ω–æ):")
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç (Stanza)..."):
            keywords = local_extract_keywords(news_text)
            relation_type = local_classify_relation(news_text)
        
        st.write(f"**üéØ –¢–∏–ø(—ã) —Å–≤—è–∑–∏:** <span style='color:#d9534f;'>{relation_type}</span>", unsafe_allow_html=True)
        st.write(f"**üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(keywords)}")

        # 4. –ò–º–∏—Ç–∞—Ü–∏—è –ü–æ–∏—Å–∫–∞
        st.header("3. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–π –Ω–æ–≤–æ—Å—Ç–∏ (–ò–ú–ò–¢–ê–¶–ò–Ø):")
        if not keywords:
            st.warning("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –ø–æ–∏—Å–∫ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        else:
            search_query = generate_search_query(keywords)
            st.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {search_query}")
            
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç 1 (–ò–º–∏—Ç–∞—Ü–∏—è)")
            snippet1 = f"–ú”ô–ª—ñ–º–µ—Ç—Ç–µ—Ä ”©—Ç–µ –Ω–∞“õ—Ç—ã, **—Å–æ–Ω–¥—ã“õ—Ç–∞–Ω** “õ–∞–±—ã–ª–¥–∞“ì–∞–Ω —à–µ—à—ñ–º–¥–µ—Ä—ñ–º—ñ–∑ –¥–∞—É—Å—ã–∑ –¥“±—Ä—ã—Å –±–æ–ª–¥—ã. –ë“±–ª {keywords[0]} “Ø—à—ñ–Ω –º–∞“£—ã–∑–¥—ã."
            st.markdown(f"> {highlight_text(snippet1, keywords)}", unsafe_allow_html=True)
            st.caption("–ò—Å—Ç–æ—á–Ω–∏–∫: example-site-1.kz")
            
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç 2 (–ò–º–∏—Ç–∞—Ü–∏—è)")
            snippet2 = f"–ñ–∞“£–∞ –∂–æ–±–∞–Ω—ã –¥–∞–π—ã–Ω–¥–∞—É“ì–∞ –∫”©–ø –µ“£–±–µ–∫ –∫–µ—Ç—Ç—ñ, **–±—ñ—Ä–∞“õ** –Ω”ô—Ç–∏–∂–µ—Å—ñ –∫“Ø—Ç–∫–µ–Ω–¥–µ–π –±–æ–ª–º–∞–¥—ã. –°–∞—Ä–∞–ø—à—ã–ª–∞—Ä {keywords[1]} —Ç—É—Ä–∞–ª—ã –∞–π—Ç—É–¥–∞."
            st.markdown(f"> {highlight_text(snippet2, keywords)}", unsafe_allow_html=True)
            st.caption("–ò—Å—Ç–æ—á–Ω–∏–∫: example-site-2.kz")